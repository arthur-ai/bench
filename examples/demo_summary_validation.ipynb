{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: \n",
    "**This demo notebook was used to generate the Test Suites and Run results recorded in the directory. Please feel free to use this as a reference when creating your own Test Suites and Test Runs.**\n",
    "\n",
    "**Running this notebook directly will result in errors. If you want to run the notebook as is, either:**\n",
    "1. Change the name of the Test Suite \n",
    "2. Delete the results from the directory\n",
    "3. Set the `BENCH_FILE_DIR` environment variable (which defaults to `./bench`) to a different directory. To do this, uncomment the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uncomment me to change the default `BENCH_FILE_DIR`\n",
    "# import os\n",
    "# os.environ['BENCH_FILE_DIR'] = 'FILL ME IN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ArthurBench: Evaluating Summaries by LLMs Choosing Better Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we evaluate the quality of three generated summaries for news articles, in reference to summaries generated by gpt-3.5-turbo. The three candidate summaries are:  \n",
    "- paraphrases of the GPT generated summaries\n",
    "- summaries generated by an open source model, trained to summarize books\n",
    "- intentionally corrupted summaries\n",
    "\n",
    "We use bench to score whether each candidate summary is better, worse, or the same quality as the reference ChatGPT summary, and to highlight common failure modes of the open source model in transferring summarization domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (summary_quality.py, line 86)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/arthur-dev/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3442\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[3], line 3\u001b[0m\n    from arthur_bench.run.testsuite import TestSuite\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Projects/arthur-bench/arthur_bench/run/testsuite.py:9\u001b[0m\n    from arthur_bench.scoring import ScoringMethod, scoring_method_class_from_string, ScoringMethodEnum\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Projects/arthur-bench/arthur_bench/scoring/__init__.py:7\u001b[0;36m\n\u001b[0;31m    from .summary_quality import SummaryQuality\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Projects/arthur-bench/arthur_bench/scoring/summary_quality.py:86\u001b[0;36m\u001b[0m\n\u001b[0;31m    @staticmethod\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from arthur_bench.run.testsuite import TestSuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a dataset of input news articles, reference summaries, and candidate summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "summary_data = pd.read_csv('news_summary/example_summaries.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Make a test suite\n",
    "\n",
    "A bench test suite consists of the inputs to the task and the target outputs. Here, we instantiate a new test suite named `compare_gpt3mapreduce`, from our data frame and indicate that the inputs are in data frame column `input_text` and the reference outputs are in column `gpt3mapreduce`. \n",
    "\n",
    "This test suite uses the scoring method `summary-qual` to evaluate future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_test_suite = TestSuite(\n",
    "    'news_summary', \n",
    "    \"summary_quality\",\n",
    "    reference_data=summary_data, \n",
    "    input_column='input_text', \n",
    "    reference_column='gpt3mapreduce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run the test\n",
    "\n",
    "Below, we create three test runs, one for each of the candidate summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_test_run = my_test_suite.run(\n",
    "    \"longt5books\", \n",
    "    candidate_data=summary_data, \n",
    "    candidate_column='longt5books'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluator prefered gpt3mapreduce over longt5books:', int(len(my_test_run.test_case_outputs) - sum([case.score for case in my_test_run.test_case_outputs])), 'out of', len(my_test_run.test_case_outputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare against other summary A/B tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_vs_rephrase = my_test_suite.run(\n",
    "    \"rephrase\", \n",
    "    candidate_data=summary_data, \n",
    "    candidate_column='chatgpt_rephrase_gpt3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluator prefered gpt3mapreduce over rephrases of gpt3mapreduce:', int(len(gpt3_vs_rephrase.test_case_outputs) - sum([case.score for case in gpt3_vs_rephrase.test_case_outputs])), 'out of', len(gpt3_vs_rephrase.test_case_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_vs_corrupt = my_test_suite.run(\n",
    "    \"corrupt\", \n",
    "    candidate_data=summary_data, \n",
    "    candidate_column='chatgpt_corrupt_longt5',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluator prefered gpt3mapreduce over corrupted longt5 summaries:', int(len(gpt3_vs_corrupt.test_case_outputs) - sum([case.score for case in gpt3_vs_corrupt.test_case_outputs])), 'out of', len(gpt3_vs_corrupt.test_case_outputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explore test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Observations about the test run results where reference was chosen over candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Typos and Hallucinations\n",
    "\n",
    "### Giuliani (1) \n",
    "\"Noelle Frank Dunphy was hired as the head of business development at **Giulinius's** new office...After one week into her job, **Giviani** flew to New York with Dunphy in order to get permission to stay in an apartment with him...\"\n",
    "\n",
    "### NBA playoffs (9) \n",
    "\"The Boston 76ers and the Philadelphia 78ers face off in a conference finals game at the Garden on Sunday, May 14...\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Not translating to this new context \n",
    "\n",
    "### Speaking book (2, 3, 8, 11, 24, 31, 38) \n",
    "\n",
    "\"In this chapter, we get a detailed look at some of the best potential players to enter the June draft and how they will fare in the event they are selected...\"\n",
    "\n",
    "\"In this chapter, we get a brief summary of what's going on with the Bieber family. We learn that Justin and Hailey are engaged and planning to have kids soon. They got married in July of last year, and they have a little wedding in October of next year\"\n",
    "\n",
    "\"The title of this chapter is \"A Florida man living beneath the ocean won't revive even after breaking a record\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Missing the main point\n",
    "\n",
    "### Nascar race (13): \n",
    "\n",
    "#### Title: NASCAR results: William Byron wins Throwback race at Darlington ahead of Kevin Harvick, Chase Elliott\\n\\nGoodyear 400 final results...\n",
    "\n",
    "\"The Goodyear 400 is a big event in the spring. It's one of the most famous races in the world, and it gets even bigger on Sunday afternoon as the field goes for a run through the state's largest race track. There's lots of good racing going on, including some classic car shows like the Dodge Grand Car Classic and the Darlington Speedway. Some newcomers will be making their first appearance, like Chase Elliott and Josh Berry. They'll all be hoping to make it to the top of the heap.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Observations about the test run results where candidate was chosen over reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Presence of the summary prompt itself fooled the evaluator: \n",
    "\n",
    "Unintentional prompt hacking (5): \"This is a very brief summary of the main point of the text. It captures all the important details in the text, and doesnt concentrate too much on tiny details. The deal with Activision has been approved by the European Union, but Britain\\'s competition authority has already veto it...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
