{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a421b10",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed1a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\" # required for executing code using the HuggingFace code_eval metric\n",
    "\n",
    "from arthur_bench.run.testsuite import TestSuite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701a837",
   "metadata": {},
   "source": [
    "# Load HumanEval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76772817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openai_humaneval (/Users/maxcembalest/.cache/huggingface/datasets/openai_humaneval/openai_humaneval/1.0.0/2955cebd73602e828fa8c0a424c594e5fab4ec863b316ca98f3d8fdb6a626e75)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7192bebe1cbc4522950656ddefe58572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_eval_data = load_dataset(\"openai_humaneval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a7bed",
   "metadata": {},
   "source": [
    "# Get `gpt-3.5-turbo` and `claude-2` solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec9d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb33e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(\n",
    "    streaming=True,\n",
    "    temperature=0.0,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ])\n",
    "claude = ChatAnthropic(\n",
    "    streaming=True,\n",
    "    temperature=0.0,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81655887",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a bot that gives answers to coding tasks only.\n",
    "If the task is a coding task, give an expert python solution.\n",
    "If the task is unrelated, give the response \"I don't know.\"\n",
    "ALWAYS mark the beginning and end of your solution with \n",
    "```python \n",
    "and \n",
    "```\n",
    "Without these markers, the code cannot be extracted. Therefore the markers are required.\n",
    "===\n",
    "Example:\n",
    "===\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def json_to_df(filename: str, threshold: float) -> pd.DataFrame\n",
    "    ''' Write a python function that loads a json file into a dataframe\n",
    "    '''\n",
    "===\n",
    "Solution:\n",
    "===\n",
    "```python\n",
    "return pd.DataFrame([json.loads(line) for line in open(filename, 'r')])\n",
    "```\n",
    "===\n",
    "Now the real task:\n",
    "===\n",
    "<text>\n",
    "===\n",
    "Solution:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ef196dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solutions():\n",
    "    code_gen_df = pd.DataFrame(code_eval_data['test'])\n",
    "    code_gen_df['chatgpt_solution'] = ['' for _ in range(len(code_gen_df))]\n",
    "    code_gen_df['claude_solution'] = ['' for _ in range(len(code_gen_df))]\n",
    "    for i in range(len(code_eval_data['test'])):\n",
    "        code_prompt = code_eval_data['test'][i]['prompt']\n",
    "        print('code prompt:\\n\\n', code_prompt)\n",
    "        filled_prompt = prompt_template.replace(\"<text>\", code_prompt)\n",
    "        print('$ chatgpt $:')\n",
    "        chatgpt_response = chatgpt([HumanMessage(content=filled_prompt)])\n",
    "        code_gen_df.loc[i, 'chatgpt_solution'] = chatgpt_response.content\n",
    "        print(\"\\n-----------\\n\")\n",
    "        print('$ claude $:')\n",
    "        claude_response = claude([HumanMessage(content=filled_prompt)])\n",
    "        code_gen_df.loc[i, 'claude_solution'] = claude_response.content\n",
    "        print(\"\\n\\n\\n>>>>>>>>>>>>>>>>\\n\\n\\n\")\n",
    "        code_gen_df.to_csv('human_eval_solutions.csv')\n",
    "# generate_solutions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182625e",
   "metadata": {},
   "source": [
    "# Load / Preprocess generated solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa72e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_df = pd.read_csv('human_eval_solutions.csv', index_col=0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3b3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract code from the solution strings\n",
    "# combine imports from prompt and solution code into candidate_outputs\n",
    "def prepare_candidates(df):\n",
    "    res = df.copy()\n",
    "    for s in ['chatgpt_solution', 'claude_solution']:\n",
    "        res[s] = [x.replace('python\\n', '').replace('```', '').replace(' def', 'def') for x in res[s]]\n",
    "    chatgpt_solns_import = []\n",
    "    claude_solns_import = []\n",
    "    for i in range(len(res)):\n",
    "        chatgpt_solns_import.append(res['prompt'][i] + '\\n' + res['chatgpt_solution'][i])\n",
    "        claude_solns_import.append(res['prompt'][i] + '\\n' + res['claude_solution'][i])\n",
    "    res['chatgpt_with_imports'] = chatgpt_solns_import\n",
    "    res['claude_with_imports'] = claude_solns_import\n",
    "    return res\n",
    "code_gen_df = prepare_candidates(code_gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ba600bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine unit test code and entrypoint name into test func script\n",
    "def prepare_test_func(df):\n",
    "    res = df.copy()\n",
    "    test_funcs = []\n",
    "    for i in range(len(res)):\n",
    "        entry_point = f\"check({code_eval_data['test'][i]['entry_point']})\"\n",
    "        test_func = \"\\n\" + code_eval_data[\"test\"][i][\"test\"] + \"\\n\" + entry_point\n",
    "        test_funcs.append(test_func)\n",
    "    res['test_func'] = test_funcs\n",
    "    return res\n",
    "code_gen_df = prepare_test_func(code_gen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d5a3640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>canonical_solution</th>\n",
       "      <th>test</th>\n",
       "      <th>entry_point</th>\n",
       "      <th>chatgpt_solution</th>\n",
       "      <th>claude_solution</th>\n",
       "      <th>chatgpt_with_imports</th>\n",
       "      <th>claude_with_imports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HumanEval/0</td>\n",
       "      <td>from typing import List\\n\\n\\ndef has_close_ele...</td>\n",
       "      <td>for idx, elem in enumerate(numbers):\\n    ...</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>has_close_elements</td>\n",
       "      <td>def has_close_elements(numbers: List[float], t...</td>\n",
       "      <td>def has_close_elements(numbers: List[float], t...</td>\n",
       "      <td>from typing import List\\n\\n\\ndef has_close_ele...</td>\n",
       "      <td>from typing import List\\n\\n\\ndef has_close_ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HumanEval/1</td>\n",
       "      <td>from typing import List\\n\\n\\ndef separate_pare...</td>\n",
       "      <td>result = []\\n    current_string = []\\n    ...</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>separate_paren_groups</td>\n",
       "      <td>def separate_paren_groups(paren_string: str) -...</td>\n",
       "      <td>def separate_paren_groups(paren_string: str) -...</td>\n",
       "      <td>from typing import List\\n\\n\\ndef separate_pare...</td>\n",
       "      <td>from typing import List\\n\\n\\ndef separate_pare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HumanEval/2</td>\n",
       "      <td>\\n\\ndef truncate_number(number: float) -&gt; floa...</td>\n",
       "      <td>return number % 1.0\\n</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>truncate_number</td>\n",
       "      <td>return number % 1\\n</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>\\n\\ndef truncate_number(number: float) -&gt; floa...</td>\n",
       "      <td>\\n\\ndef truncate_number(number: float) -&gt; floa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HumanEval/3</td>\n",
       "      <td>from typing import List\\n\\n\\ndef below_zero(op...</td>\n",
       "      <td>balance = 0\\n\\n    for op in operations:\\n...</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>below_zero</td>\n",
       "      <td>def below_zero(operations: List[int]) -&gt; bool:...</td>\n",
       "      <td>def below_zero(operations: List[int]) -&gt; bool:...</td>\n",
       "      <td>from typing import List\\n\\n\\ndef below_zero(op...</td>\n",
       "      <td>from typing import List\\n\\n\\ndef below_zero(op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HumanEval/4</td>\n",
       "      <td>from typing import List\\n\\n\\ndef mean_absolute...</td>\n",
       "      <td>mean = sum(numbers) / len(numbers)\\n    re...</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>mean_absolute_deviation</td>\n",
       "      <td>def mean_absolute_deviation(numbers: List[floa...</td>\n",
       "      <td>def mean_absolute_deviation(numbers: List[floa...</td>\n",
       "      <td>from typing import List\\n\\n\\ndef mean_absolute...</td>\n",
       "      <td>from typing import List\\n\\n\\ndef mean_absolute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>HumanEval/69</td>\n",
       "      <td>\\ndef search(lst):\\n    '''\\n    You are given...</td>\n",
       "      <td>frq = [0] * (max(lst) + 1)\\n    for i in l...</td>\n",
       "      <td>def check(candidate):\\n\\n    # manually genera...</td>\n",
       "      <td>search</td>\n",
       "      <td>def search(lst):\\n    freq_dict = {}\\n    for ...</td>\n",
       "      <td>def search(lst):\\n    freq = {}\\n    for num i...</td>\n",
       "      <td>\\ndef search(lst):\\n    '''\\n    You are given...</td>\n",
       "      <td>\\ndef search(lst):\\n    '''\\n    You are given...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>HumanEval/70</td>\n",
       "      <td>\\ndef strange_sort_list(lst):\\n    '''\\n    Gi...</td>\n",
       "      <td>res, switch = [], True\\n    while lst:\\n  ...</td>\n",
       "      <td>def check(candidate):\\n\\n    # Check some simp...</td>\n",
       "      <td>strange_sort_list</td>\n",
       "      <td>def strange_sort_list(lst):\\n    sorted_lst = ...</td>\n",
       "      <td>def strange_sort_list(lst):\\n    if not lst:\\n...</td>\n",
       "      <td>\\ndef strange_sort_list(lst):\\n    '''\\n    Gi...</td>\n",
       "      <td>\\ndef strange_sort_list(lst):\\n    '''\\n    Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>HumanEval/71</td>\n",
       "      <td>\\ndef triangle_area(a, b, c):\\n    '''\\n    Gi...</td>\n",
       "      <td>if a + b &lt;= c or a + c &lt;= b or b + c &lt;= a:...</td>\n",
       "      <td>def check(candidate):\\n\\n    # Check some simp...</td>\n",
       "      <td>triangle_area</td>\n",
       "      <td>import math\\n\\ndef triangle_area(a, b, c):\\n  ...</td>\n",
       "      <td>def triangle_area(a, b, c):\\n    if a + b &gt; c ...</td>\n",
       "      <td>\\ndef triangle_area(a, b, c):\\n    '''\\n    Gi...</td>\n",
       "      <td>\\ndef triangle_area(a, b, c):\\n    '''\\n    Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>HumanEval/72</td>\n",
       "      <td>\\ndef will_it_fly(q,w):\\n    '''\\n    Write a ...</td>\n",
       "      <td>if sum(q) &gt; w:\\n        return False\\n\\n  ...</td>\n",
       "      <td>def check(candidate):\\n\\n    # Check some simp...</td>\n",
       "      <td>will_it_fly</td>\n",
       "      <td>def will_it_fly(q, w):\\n    if q == q[::-1] an...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>\\ndef will_it_fly(q,w):\\n    '''\\n    Write a ...</td>\n",
       "      <td>\\ndef will_it_fly(q,w):\\n    '''\\n    Write a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>HumanEval/73</td>\n",
       "      <td>\\ndef smallest_change(arr):\\n    \"\"\"\\n    Give...</td>\n",
       "      <td>ans = 0\\n    for i in range(len(arr) // 2)...</td>\n",
       "      <td>def check(candidate):\\n\\n    # Check some simp...</td>\n",
       "      <td>smallest_change</td>\n",
       "      <td>def smallest_change(arr):\\n    n = len(arr)\\n ...</td>\n",
       "      <td>def smallest_change(arr):\\n    n = len(arr)\\n ...</td>\n",
       "      <td>\\ndef smallest_change(arr):\\n    \"\"\"\\n    Give...</td>\n",
       "      <td>\\ndef smallest_change(arr):\\n    \"\"\"\\n    Give...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         task_id                                             prompt  \\\n",
       "0    HumanEval/0  from typing import List\\n\\n\\ndef has_close_ele...   \n",
       "1    HumanEval/1  from typing import List\\n\\n\\ndef separate_pare...   \n",
       "2    HumanEval/2  \\n\\ndef truncate_number(number: float) -> floa...   \n",
       "3    HumanEval/3  from typing import List\\n\\n\\ndef below_zero(op...   \n",
       "4    HumanEval/4  from typing import List\\n\\n\\ndef mean_absolute...   \n",
       "..           ...                                                ...   \n",
       "69  HumanEval/69  \\ndef search(lst):\\n    '''\\n    You are given...   \n",
       "70  HumanEval/70  \\ndef strange_sort_list(lst):\\n    '''\\n    Gi...   \n",
       "71  HumanEval/71  \\ndef triangle_area(a, b, c):\\n    '''\\n    Gi...   \n",
       "72  HumanEval/72  \\ndef will_it_fly(q,w):\\n    '''\\n    Write a ...   \n",
       "73  HumanEval/73  \\ndef smallest_change(arr):\\n    \"\"\"\\n    Give...   \n",
       "\n",
       "                                   canonical_solution  \\\n",
       "0       for idx, elem in enumerate(numbers):\\n    ...   \n",
       "1       result = []\\n    current_string = []\\n    ...   \n",
       "2                               return number % 1.0\\n   \n",
       "3       balance = 0\\n\\n    for op in operations:\\n...   \n",
       "4       mean = sum(numbers) / len(numbers)\\n    re...   \n",
       "..                                                ...   \n",
       "69      frq = [0] * (max(lst) + 1)\\n    for i in l...   \n",
       "70      res, switch = [], True\\n    while lst:\\n  ...   \n",
       "71      if a + b <= c or a + c <= b or b + c <= a:...   \n",
       "72      if sum(q) > w:\\n        return False\\n\\n  ...   \n",
       "73      ans = 0\\n    for i in range(len(arr) // 2)...   \n",
       "\n",
       "                                                 test  \\\n",
       "0   \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "1   \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "2   \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "3   \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "4   \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "..                                                ...   \n",
       "69  def check(candidate):\\n\\n    # manually genera...   \n",
       "70  def check(candidate):\\n\\n    # Check some simp...   \n",
       "71  def check(candidate):\\n\\n    # Check some simp...   \n",
       "72  def check(candidate):\\n\\n    # Check some simp...   \n",
       "73  def check(candidate):\\n\\n    # Check some simp...   \n",
       "\n",
       "                entry_point  \\\n",
       "0        has_close_elements   \n",
       "1     separate_paren_groups   \n",
       "2           truncate_number   \n",
       "3                below_zero   \n",
       "4   mean_absolute_deviation   \n",
       "..                      ...   \n",
       "69                   search   \n",
       "70        strange_sort_list   \n",
       "71            triangle_area   \n",
       "72              will_it_fly   \n",
       "73          smallest_change   \n",
       "\n",
       "                                     chatgpt_solution  \\\n",
       "0   def has_close_elements(numbers: List[float], t...   \n",
       "1   def separate_paren_groups(paren_string: str) -...   \n",
       "2                                 return number % 1\\n   \n",
       "3   def below_zero(operations: List[int]) -> bool:...   \n",
       "4   def mean_absolute_deviation(numbers: List[floa...   \n",
       "..                                                ...   \n",
       "69  def search(lst):\\n    freq_dict = {}\\n    for ...   \n",
       "70  def strange_sort_list(lst):\\n    sorted_lst = ...   \n",
       "71  import math\\n\\ndef triangle_area(a, b, c):\\n  ...   \n",
       "72  def will_it_fly(q, w):\\n    if q == q[::-1] an...   \n",
       "73  def smallest_change(arr):\\n    n = len(arr)\\n ...   \n",
       "\n",
       "                                      claude_solution  \\\n",
       "0   def has_close_elements(numbers: List[float], t...   \n",
       "1   def separate_paren_groups(paren_string: str) -...   \n",
       "2                                       I don't know.   \n",
       "3   def below_zero(operations: List[int]) -> bool:...   \n",
       "4   def mean_absolute_deviation(numbers: List[floa...   \n",
       "..                                                ...   \n",
       "69  def search(lst):\\n    freq = {}\\n    for num i...   \n",
       "70  def strange_sort_list(lst):\\n    if not lst:\\n...   \n",
       "71  def triangle_area(a, b, c):\\n    if a + b > c ...   \n",
       "72                                      I don't know.   \n",
       "73  def smallest_change(arr):\\n    n = len(arr)\\n ...   \n",
       "\n",
       "                                 chatgpt_with_imports  \\\n",
       "0   from typing import List\\n\\n\\ndef has_close_ele...   \n",
       "1   from typing import List\\n\\n\\ndef separate_pare...   \n",
       "2   \\n\\ndef truncate_number(number: float) -> floa...   \n",
       "3   from typing import List\\n\\n\\ndef below_zero(op...   \n",
       "4   from typing import List\\n\\n\\ndef mean_absolute...   \n",
       "..                                                ...   \n",
       "69  \\ndef search(lst):\\n    '''\\n    You are given...   \n",
       "70  \\ndef strange_sort_list(lst):\\n    '''\\n    Gi...   \n",
       "71  \\ndef triangle_area(a, b, c):\\n    '''\\n    Gi...   \n",
       "72  \\ndef will_it_fly(q,w):\\n    '''\\n    Write a ...   \n",
       "73  \\ndef smallest_change(arr):\\n    \"\"\"\\n    Give...   \n",
       "\n",
       "                                  claude_with_imports  \n",
       "0   from typing import List\\n\\n\\ndef has_close_ele...  \n",
       "1   from typing import List\\n\\n\\ndef separate_pare...  \n",
       "2   \\n\\ndef truncate_number(number: float) -> floa...  \n",
       "3   from typing import List\\n\\n\\ndef below_zero(op...  \n",
       "4   from typing import List\\n\\n\\ndef mean_absolute...  \n",
       "..                                                ...  \n",
       "69  \\ndef search(lst):\\n    '''\\n    You are given...  \n",
       "70  \\ndef strange_sort_list(lst):\\n    '''\\n    Gi...  \n",
       "71  \\ndef triangle_area(a, b, c):\\n    '''\\n    Gi...  \n",
       "72  \\ndef will_it_fly(q,w):\\n    '''\\n    Write a ...  \n",
       "73  \\ndef smallest_change(arr):\\n    \"\"\"\\n    Give...  \n",
       "\n",
       "[74 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_gen_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e91dfd",
   "metadata": {},
   "source": [
    "# Create CodeEval test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061e1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_suite = TestSuite(\n",
    "    'humaneval', \n",
    "    \"code_eval\",\n",
    "    reference_data=code_gen_df, \n",
    "    input_column='prompt', \n",
    "    reference_column='test_func')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f796183",
   "metadata": {},
   "source": [
    "# Evaluate ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e80dd3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxcembalest/Desktop/arthur/arthur-ai/bench/arthur_bench/scoring/code_eval.py:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.scorer = load_metric(\"code_eval\") # type: ignore\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:13<00:00,  5.59it/s]\n"
     ]
    }
   ],
   "source": [
    "my_test_run_chatgpt = my_test_suite.run(\n",
    "    \"chatgpt\", \n",
    "    candidate_data=code_gen_df, \n",
    "    candidate_column='chatgpt_solution'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72032088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS@1 SCORE ON HUMANEVAL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'55.410000000000004%'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"PASS@1 SCORE ON HUMANEVAL\")\n",
    "str(np.round(np.array([\n",
    "    my_test_run_chatgpt.test_case_outputs[i].score \n",
    "    for i in range(len(my_test_run_chatgpt.test_case_outputs))]\n",
    ").mean(), 4) * 100) + \"%\"\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47924b89",
   "metadata": {},
   "source": [
    "# Evaluate ChatGPT ( providing `import` statements to help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a648804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:13<00:00,  5.47it/s]\n"
     ]
    }
   ],
   "source": [
    "my_test_run_chatgpt_with_imports = my_test_suite.run(\n",
    "    \"chatgpt_with_imports\", \n",
    "    candidate_data=code_gen_df, \n",
    "    candidate_column='chatgpt_with_imports'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7399081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS@1 SCORE ON HUMANEVAL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'75.68%'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"PASS@1 SCORE ON HUMANEVAL\")\n",
    "str(np.round(np.array([\n",
    "    my_test_run_chatgpt_with_imports.test_case_outputs[i].score \n",
    "    for i in range(len(my_test_run_chatgpt_with_imports.test_case_outputs))]\n",
    ").mean(), 4) * 100) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0878be",
   "metadata": {},
   "source": [
    "# Evaluate Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "641c4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_run_claude = my_test_suite.run(\n",
    "    \"claude\", \n",
    "    candidate_data=code_gen_df, \n",
    "    candidate_column='claude_solution'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "185a213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS@1 SCORE ON HUMANEVAL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5.41%'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"PASS@1 SCORE ON HUMANEVAL\")\n",
    "str(np.round(np.array([\n",
    "    my_test_run_claude.test_case_outputs[i].score \n",
    "    for i in range(len(my_test_run_claude.test_case_outputs))]\n",
    ").mean(), 4) * 100) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a56e1a5",
   "metadata": {},
   "source": [
    "# Evaluate Claude ( providing `import` statements to help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eaca69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:13<00:00,  5.36it/s]\n"
     ]
    }
   ],
   "source": [
    "my_test_run_claude_with_imports = my_test_suite.run(\n",
    "    \"claude_with_imports\", \n",
    "    candidate_data=code_gen_df, \n",
    "    candidate_column='claude_with_imports'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec587cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS@1 SCORE ON HUMANEVAL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'18.92%'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"PASS@1 SCORE ON HUMANEVAL\")\n",
    "str(np.round(np.array([\n",
    "    my_test_run_claude_with_imports.test_case_outputs[i].score \n",
    "    for i in range(len(my_test_run_claude_with_imports.test_case_outputs))]\n",
    ").mean(), 4) * 100) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee6920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
