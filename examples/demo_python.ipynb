{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25ed96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from arthur_bench.run.testsuite import TestSuite\n",
    "from arthur_bench.scoring import PythonUnitTesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec3547",
   "metadata": {},
   "source": [
    "# Load python coding benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712615dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openai_humaneval (/Users/maxcembalest/.cache/huggingface/datasets/openai_humaneval/openai_humaneval/1.0.0/2955cebd73602e828fa8c0a424c594e5fab4ec863b316ca98f3d8fdb6a626e75)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8be3c3ced4f4d39a5bda8ee93090074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "humaneval_code_dataset = load_dataset(\"openai_humaneval\")\n",
    "humaneval_df = pd.DataFrame(humaneval_code_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517354ff",
   "metadata": {},
   "source": [
    "# Generate solutions from GPT and Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bf0e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "anthropic = Anthropic()\n",
    "def chatgpt(input_text):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[\n",
    "            {\"role\" : \"system\", \"content\" : \"You are a helpful assistant.\"},\n",
    "            {\"role\" : \"user\", \"content\" : input_text}\n",
    "        ]\n",
    "    )['choices'][0]['message']['content']\n",
    "\n",
    "def claude(input_text):\n",
    "    return anthropic.completions.create(\n",
    "        model=\"claude-2\",\n",
    "        max_tokens_to_sample=300,\n",
    "        prompt=f\"{HUMAN_PROMPT} {input_text} {AI_PROMPT}\",\n",
    "    ).completion\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a bot that gives answers to coding tasks only.\n",
    "If the task is a coding task, give an expert python solution.\n",
    "If the task is unrelated, give the response \"I don't know.\"\n",
    "ALWAYS mark the beginning and end of your solution with \n",
    "```python \n",
    "and \n",
    "```\n",
    "Without these markers, the code cannot be extracted. Therefore the markers are required.\n",
    "===\n",
    "<text>\n",
    "===\n",
    "Solution:\n",
    "\"\"\"\n",
    "\n",
    "def generate_solutions(df):\n",
    "    res = df.copy()\n",
    "    res['chatgpt_solution'] = ['' for _ in range(len(res))]\n",
    "    res['claude_solution'] = ['' for _ in range(len(res))]\n",
    "    for i in range(len(df)):\n",
    "        code_prompt = df[i]['prompt']\n",
    "        filled_prompt = prompt_template.replace(\"<text>\", code_prompt)\n",
    "        chatgpt_response = chatgpt(filled_prompt)\n",
    "        claude_response = claude(filled_prompt)\n",
    "        bar = \"-----------\"\n",
    "        print(f\"code prompt:\\n\\n{code_prompt}\\n{bar}\\nchatgpt:\\n{chatgpt_response}\\n{bar}\\nclaude:\\n{claude_response}\\n\\n{bar}{bar}\\n\\n\")\n",
    "        code_gen_df.loc[i, 'chatgpt_solution'] = chatgpt_response\n",
    "        code_gen_df.loc[i, 'claude_solution'] = claude_response\n",
    "        code_gen_df.to_csv('human_eval_solutions.csv')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c481a",
   "metadata": {},
   "source": [
    "Uncomment and run this cell to generate solutions to the coding benchmark dataset from `chatgpt` and `claude`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1e1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_solutions(humaneval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e3047",
   "metadata": {},
   "source": [
    "# Evaluate the LLM-generated coding solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f3bb76",
   "metadata": {},
   "source": [
    "Load the LLM-generated solutions to the humaneval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed91006",
   "metadata": {},
   "outputs": [],
   "source": [
    "humaneval_results = pd.read_csv(\"human_eval_solutions.csv\", index_col=0).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f55c0f",
   "metadata": {},
   "source": [
    "Parse the part of the output which is python from between the  \\` \\` \\` python... \\` \\` \\`  markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84117972",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_python = lambda x : x.replace('python\\n', '').replace('```', '').replace(' def', 'def')\n",
    "chatgpt_solutions = list(humaneval_results.chatgpt_solution.apply(extract_python))\n",
    "claude_solutions = list(humaneval_results.claude_solution.apply(extract_python))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d3e8a",
   "metadata": {},
   "source": [
    "### Create the test suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51cd67",
   "metadata": {},
   "source": [
    "First we create the list of unit tests from the humaneval dataset\n",
    "\n",
    "Each unit test should contain `def check(candidate):...` and then at the end of the unit test invoke the unit test with `check(<candidate_function_name>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbcb8d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_tests = [\n",
    "    f'\\n{humaneval_df.loc[i][\"test\"]}\\ncheck({humaneval_df.loc[i][\"entry_point\"]})' \n",
    "    for i in range(len(humaneval_df))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a8707",
   "metadata": {},
   "source": [
    "We create a PythonUnitTesting scorer with our unit tests as a parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e469fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_scorer = PythonUnitTesting(unit_tests=unit_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1c4d1",
   "metadata": {},
   "source": [
    "Now we can create our test suite with our scorer, input prompts, and the golden/canonical solutions from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8395a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_suite = TestSuite(\n",
    "    \"humaneval\", \n",
    "    python_scorer, \n",
    "    input_text_list=list(humaneval_df.prompt.values), \n",
    "    reference_output_list=list(humaneval_df.canonical_solution.values),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e21fe6",
   "metadata": {},
   "source": [
    "### Run the test suite on the `chatgpt` and `claude` solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f55243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_scores = lambda scores : f\"{sum(scores)/len(scores) * 100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba41cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 164/164 [00:37<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "run_chatgpt = python_suite.run(\n",
    "    \"chatgpt\", \n",
    "    candidate_output_list=chatgpt_solutions, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b112779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT: 54.268292682926834%\n"
     ]
    }
   ],
   "source": [
    "print(\"ChatGPT:\", report_scores([x.score for x in run_chatgpt.test_cases]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd8bb480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 164/164 [00:35<00:00,  4.57it/s]\n"
     ]
    }
   ],
   "source": [
    "run_claude = python_suite.run(\n",
    "    \"claude\", \n",
    "    candidate_output_list=claude_solutions, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c545d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude: 48.170731707317074%\n"
     ]
    }
   ],
   "source": [
    "print(\"Claude:\", report_scores([x.score for x in run_claude.test_cases]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef8918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
