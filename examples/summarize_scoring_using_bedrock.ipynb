{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68397145-e13d-4e6d-bb0a-35944409c333",
   "metadata": {},
   "source": [
    "# Summarization Scoring using AWS Bedrock\n",
    "\n",
    "In this example notebook, we will be comparing how different models in AWS Bedrock perform at summarization tasks using **Arthur Bench**. \n",
    "\n",
    "The overall summarization comparison is setup as a **Bench TestSuite**, and each model is compared head-to-head against every other model\n",
    "in a **Bench TestRun**. Bench provides the ability to view these comparisons in the provided User Interface, as well as access statistics\n",
    "from the Test and TestRuns themselves for further analysis. For example, in the notebook below we provide a means for using the ELO Scoring\n",
    "Algorithm to determine which model performs best at this summarization task. \n",
    "                                                                                                                 \n",
    "The task is to summarize 49 News Articles, and comparison is done using ChatGPT 3.5 Turbo (see summary_quality.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97dda90-22a7-4a8d-8cf5-e2948d35f8af",
   "metadata": {},
   "source": [
    "## 1. Setup AWS Bedrock Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375f148-b8f3-4a6d-8a10-f4e96e97ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Authentication is handled using the AWS_PROFILE environment variable. Check the AWS Boto3 documentation and the provided\n",
    "utility library for connecting to Bedrock for additional information\n",
    "\"\"\"\n",
    "from bedrock_client import client\n",
    "\n",
    "bedrock_runtime = client.get_bedrock_client(region=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3eeae-afa0-474b-974b-bcaa716d83c8",
   "metadata": {},
   "source": [
    "## 2. Load the data and prepare it for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee14bb-017d-4057-a406-1f0cc60e5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "articles = []\n",
    "\n",
    "with open(\"data/news_summary/example_summaries.csv\", \"r\") as f:\n",
    "    dr = csv.DictReader(f)\n",
    "    for row in dr: \n",
    "        articles.append(row[\"input_text\"])\n",
    "\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d4a64-1fc0-493c-b7d5-556d4174313e",
   "metadata": {},
   "source": [
    "## 3. Generate inferences (summaries) for the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d18def-3fe4-4eba-aacc-0882cf123bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt = \"\"\"\\\n",
    "Summarize the following news document down to its most important points in less than 250 words.\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "def generate_summary_from_llama(model_id, article): \n",
    "    body = json.dumps({\"prompt\": prompt.format(article)})\n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"generation\")\n",
    "\n",
    "\n",
    "def generate_summary_from_titan(model_id, article):\n",
    "    body = json.dumps({\"inputText\": prompt.format(article)})\n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"results\")[0].get(\"outputText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70fcc4-5396-4733-9a93-785c6a53b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (\n",
    "    \"meta.llama2-13b-chat-v1\",\n",
    "    \"meta.llama2-70b-chat-v1\",\n",
    "    \"amazon.titan-text-lite-v1\"\n",
    ")\n",
    "\n",
    "summaries = {}\n",
    "summaries[\"input\"] = articles\n",
    "\n",
    "for model in models:\n",
    "    summaries.setdefault(model, [])\n",
    "    for i, article in enumerate(articles): \n",
    "        try:\n",
    "            if \"meta\" in model:\n",
    "                summary = generate_summary_from_llama(model, article)\n",
    "            elif \"amazon\" in model:\n",
    "                summary = generate_summary_from_titan(model, article)\n",
    "            else:\n",
    "                print(f\"Unable to determine what {model} is\")\n",
    "                continue\n",
    "                \n",
    "            summaries[model].append(summary)\n",
    "        except:\n",
    "            print(f\"Couldn't generate summary for article {i} using model {model}\")\n",
    "            summaries[model].append(\"\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649fcb0-7e11-4002-9bed-e9c2185112a1",
   "metadata": {},
   "source": [
    "### 3.5 Save (and load) inferences to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad5571-e159-4559-883c-06eeb9f81f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"summaries.pkl\", \"wb\") as f:\n",
    "    pickle.dump(summaries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d779158-1319-4baf-94a7-b742e401133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('summaries.pkl', 'rb') as f:\n",
    "    summaries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54947f69-7b97-467f-bf09-86eac0c8198d",
   "metadata": {},
   "source": [
    "## 4. Setup a Bench TestSuite and run Bench TestRuns\n",
    "\n",
    "At this point, make sure you've set the `BENCH_FILE_DIR` environment variable. \n",
    "\n",
    "See the [Quickstart Guide](https://bench.readthedocs.io/en/latest/quickstart.html#view-results-in-local-ui) for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511884e-dc11-4c06-a4be-6c8cc5ae61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arthur_bench.run.testsuite import TestSuite\n",
    "import os \n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"REPLACE_ME\"\n",
    "\n",
    "\n",
    "suite = TestSuite(\n",
    "    \"News Summarization Using Bedrock\", \n",
    "    'summary_quality',\n",
    "    input_text_list=summaries[\"input\"],\n",
    "    reference_output_list=summaries[\"amazon.titan-text-lite-v1\"]  # Use Titan's summarization as our \"reference\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cd40d-80dc-4ad2-aded-a7a491a33887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare titan's summarization against llama 13b\n",
    "run_llama_13b_compare = suite.run(\n",
    "    run_name=\"titan_vs_llama_13b\",\n",
    "    candidate_output_list=summaries[\"meta.llama2-13b-chat-v1\"]\n",
    ")\n",
    "\n",
    "# Compare titan's summarization against llama 70b\n",
    "run_llama_13b_compare = suite.run(\n",
    "    run_name=\"titan_vs_llama_70b\",\n",
    "    candidate_output_list=summaries[\"meta.llama2-70b-chat-v1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fe8fb-d663-4170-96d4-288b214c115d",
   "metadata": {},
   "source": [
    "# 5. Open the Bench UI and view your results\n",
    "\n",
    "From your command line, enter the `bench` command and load the UI from the URL output in your terminal window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd1d19-aa92-4818-bbb2-be5bdce980ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
