
Bench Documentation Home
========================
Bench is an open source python library for evaluating generative text models for production use cases. 
The Bench evaluation framework is useful for judging and tracking model generations for a specific task, such as:

* Comparing the generations of a free, open source model to the generations of a paid model provider. In this `example <https://github.com/arthur-ai/bench/blob/main/examples/demo_summary_validation.ipynb>`_, we compare news articles summaries of an open source model loaded from huggingface with summaries generated by gpt-3.
  
* Comparing the generations of two different model providers. In this example, we compare the generations of OpenAI and Cohere on a product manual question answering task. 
  
* Comparing the generations of two different global prompt templates. In this `example <https://github.com/arthur-ai/bench/blob/main/examples/demo_bertscore.ipynb>`_, we compare an out of the box prompt, with an engineered prompt for a python QA task.
  
* Comparing the generations of a new model version with the existing production model.
 
Get started by visiting our :doc:`quickstart page<quickstart>`, reading about :doc:`key concepts <concepts>`,  or viewing our `Github <https://github.com/arthur-ai/bench>`_.

Contents
========
.. toctree::
   :maxdepth: 2

   self
   quickstart.md
   concepts.md
   guides
   sdk_reference
   contributing.md


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
