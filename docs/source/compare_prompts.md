## Compare Generation Settings

Outline

In this guide we compare LLM-generated answers to questions using two different prompts. 

We use the BERTScore scoring method to measure how much the improved prompt brings the LLM-generated answer closer to the desired reference output.

- env
- test suite data
- get LLM responses
- make test suite
- run tests on each set of responses
- view results


env
```
export OPENAI_API_KEY="sk-..."
```

test suite data

```python

```


LLM responses

```python

```

make test suite

```python

```

run tests on each set of responses

```python

```

view results

```python

```
